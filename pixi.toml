[project]
channels = ["conda-forge"]
platforms = ["linux-64", "osx-64", "osx-arm64"]
name = "samplerlab"

[dependencies]
python = "~=3.12"
arviz = ">=0.18.0"
graphviz = "*"
jupyterlab = ">=4.2"
jax = ">=0.4.31"
numba = ">=0.60"
numpy = "<=2.0"
pandas = ">=2.1"
polars = ">=1.1.0"
pyprojroot = ">=0.3.0"
scipy = ">=1.13.0"
seaborn = ">=0.13.0"
numpyro = ">=0.15.0"
pytest = ">=8.2"
threadpoolctl = ">=3.5.0,<3.6"
python-graphviz = "*"
pytensor = ">=2.22.1"
blackjax = ">=1.2.0"
pymc = ">=5.16.2"
nutpie = ">=0.13.1"
watermark = ">=2.4.3,<3"
quarto = ">=1.4.557,<2"

[pypi-dependencies]
samplerlab = { path = ".", editable = true }

[target.osx.dependencies]
libblas = { version = "*", build = "*accelerate" }

[target.linux-64.dependencies]
libblas = { version = "*", build = "*mkl" }
intel-cmplr-lib-rt = ">=2024.2.0,<2025"

[activation.env]
XLA_PYTHON_CLIENT_PREALLOCATE = "false"
# Force 4 cpu devices, or jax will not use pmap correctly
XLA_FLAGS = "--xla_force_host_platform_device_count=4"

# Prevent jax from preallocating most of the GPU to avoid
# OOM conditions with multiple clients running concurrently.
# The tbb threading layer seems to work better with mutiple
# chains sampling concurrently and using blas functions.
[feature.cuda12]
platforms = ["linux-64"]
system-requirements = { cuda = "12" }

[feature.cuda12.dependencies]
jaxlib = { version = "*", build = "*cuda12*" }
cuda-version = "12.*"

[environments]
default = { features = [] }
cuda12 = { features = ["cuda12"] }

[tasks]
list-jax-devices = { cmd = "python -c 'import jax; jax.config.update(\"jax_default_device\", jax.devices(\"cpu\")[0]); print(jax.devices())'" }
jupyterlab = { cmd = "jupyter lab --no-browser" }
benchmark = { cmd = "python -m samplerlab" }
show-env = { cmd = "env" }
